{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Tree Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "\n",
    "# file\n",
    "import os\n",
    "\n",
    "\n",
    "# folder\n",
    "folder_input = '02_input'\n",
    "\n",
    "\n",
    "# Note about package and environment:\n",
    "\n",
    "# 1. Environment: \n",
    "# Choose myenv as kernel. This was set up following guideline from vs code. A command line was typed in minicoda, enabling vs code to use packages from miniconda.\n",
    "# Ref: https://code.visualstudio.com/docs/datascience/data-science-tutorial#_prerequisites\n",
    "\n",
    "# 2. Add packages to the environment already created:\n",
    "# Open the minicoda > anaconda prompt > type 'conda install -n <env_name> <package>' > then the package can be used in myenv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
       "       'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n",
       "       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
       "       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
       "       'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original data\n",
    "org = pd.read_csv(os.path.join('..', folder_input, 'WA_Fn-UseC_-Telco-Customer-Churn.csv'))\n",
    "\n",
    "org.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   gender            7043 non-null   object \n",
      " 1   SeniorCitizen     7043 non-null   int64  \n",
      " 2   Partner           7043 non-null   object \n",
      " 3   Dependents        7043 non-null   object \n",
      " 4   tenure            7043 non-null   int64  \n",
      " 5   PhoneService      7043 non-null   object \n",
      " 6   MultipleLines     7043 non-null   object \n",
      " 7   InternetService   7043 non-null   object \n",
      " 8   OnlineSecurity    7043 non-null   object \n",
      " 9   OnlineBackup      7043 non-null   object \n",
      " 10  DeviceProtection  7043 non-null   object \n",
      " 11  TechSupport       7043 non-null   object \n",
      " 12  StreamingTV       7043 non-null   object \n",
      " 13  StreamingMovies   7043 non-null   object \n",
      " 14  Contract          7043 non-null   object \n",
      " 15  PaperlessBilling  7043 non-null   object \n",
      " 16  PaymentMethod     7043 non-null   object \n",
      " 17  MonthlyCharges    7043 non-null   float64\n",
      " 18  Churn             7043 non-null   int64  \n",
      "dtypes: float64(1), int64(3), object(15)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# data cleansing\n",
    "\n",
    "# target\n",
    "cleaned_data = org.copy()\n",
    "cleaned_data['Churn'] = cleaned_data['Churn'].str.replace(\"Yes\", \"1\", case=False, regex=False)\n",
    "cleaned_data['Churn'] = cleaned_data['Churn'].str.replace(\"No\", \"0\", case=False, regex=False)\n",
    "cleaned_data = cleaned_data.astype({'Churn': 'int64'})\n",
    "\n",
    "\n",
    "# # Change column type to category for columns: 'gender', 'SeniorCitizen' and 15 other columns\n",
    "# cleaned_data = org.astype({\n",
    "#     'gender': 'category', \n",
    "#     'SeniorCitizen': 'category', \n",
    "#     'Partner': 'category', \n",
    "#     'Dependents': 'category', \n",
    "#     'PhoneService': 'category', \n",
    "#     'MultipleLines': 'category', \n",
    "#     'InternetService': 'category', \n",
    "#     'OnlineSecurity': 'category', \n",
    "#     'OnlineBackup': 'category', \n",
    "#     'DeviceProtection': 'category', \n",
    "#     'TechSupport': 'category', \n",
    "#     'StreamingTV': 'category', \n",
    "#     'StreamingMovies': 'category', \n",
    "#     'Contract': 'category', \n",
    "#     'PaperlessBilling': 'category', \n",
    "#     'PaymentMethod': 'category', \n",
    "#     'Churn': 'category'\n",
    "#     })\n",
    "\n",
    "\n",
    "# drop useless cols\n",
    "cleaned_data = cleaned_data.drop(\n",
    "    labels=['TotalCharges', 'customerID'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# inspect data structure\n",
    "cleaned_data.head()\n",
    "cleaned_data.describe()\n",
    "cleaned_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. random forest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# seperate target from predictors\n",
    "X = cleaned_data.copy()            # use copy, not to affect original dataset\n",
    "y = X.pop('Churn')        # y = \"churn\", and delete churn from X\n",
    "\n",
    "# train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocessing steps\n",
    "\n",
    "# select categorical & numericial columns\n",
    "categorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "\n",
    "# pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "# numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# To be read\n",
    "# rf ref: https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/\n",
    "# kaggle ref for classification: https://www.kaggle.com/code/prashant111/random-forest-classifier-tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7518455423055083"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and evaluate pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', rf_model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_test)\n",
    "\n",
    "# # check the unique value of the output - 1, 0, so it's classification output\n",
    "# np.unique(preds)\n",
    "\n",
    "# Evaluate the model\n",
    "# PS: Accuracy score = % of accuracy in prediction,\n",
    "#PS: accuracy is not a great measure of classifier performance when the classes are imbalanced\n",
    "accuracy_score(y_test, preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1120,  178],\n",
       "       [ 259,  204]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "# Get and reshape confusion matrix data\n",
    "confusion_matrix(y_test, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85      1298\n",
      "           1       0.59      0.46      0.52       463\n",
      "\n",
      "    accuracy                           0.77      1761\n",
      "   macro avg       0.71      0.67      0.69      1761\n",
      "weighted avg       0.76      0.77      0.76      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "# View the classification report for test data and predictions\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# Precision = acurracy of the positive prediction (ie TP/(TP + FP))\n",
    "# Recall = % of positive being correctly identified (ie TP/(TP+FN))\n",
    "# percent of positive predictions were correct (ie 2*(Recall * Precision) / (Recall + Precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model without pipeline to get feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03350508, 0.03325889, 0.02679477, 0.02645615, 0.02269356,\n",
       "       0.02207425, 0.00495815, 0.0055726 , 0.03050596, 0.00550736,\n",
       "       0.0298771 , 0.01883244, 0.04029614, 0.00263534, 0.04351384,\n",
       "       0.0024463 , 0.02119201, 0.03063093, 0.00389934, 0.02379819,\n",
       "       0.03293993, 0.00542439, 0.02635651, 0.04270106, 0.00159131,\n",
       "       0.0212643 , 0.02796514, 0.0027891 , 0.02840908, 0.02722069,\n",
       "       0.00212311, 0.02765094, 0.10296754, 0.01969152, 0.02738032,\n",
       "       0.02643403, 0.02603606, 0.02426284, 0.02672168, 0.04991253,\n",
       "       0.02170955])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the feature scores\n",
    "# TBC: Seem no easy way to get feature importance from pipeline\n",
    "my_pipeline.steps[1][1].feature_importances_\n",
    "\n",
    "# ref: https://stackoverflow.com/questions/38787612/how-to-extract-feature-importances-from-an-sklearn-pipeline#comment91594552_38788087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(transformers=[('cat',\n",
      "                                 Pipeline(steps=[('onehot',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                 ['gender', 'Partner', 'Dependents',\n",
      "                                  'PhoneService', 'MultipleLines',\n",
      "                                  'InternetService', 'OnlineSecurity',\n",
      "                                  'OnlineBackup', 'DeviceProtection',\n",
      "                                  'TechSupport', 'StreamingTV',\n",
      "                                  'StreamingMovies', 'Contract',\n",
      "                                  'PaperlessBilling', 'PaymentMethod'])])\n"
     ]
    }
   ],
   "source": [
    "# cannot figure out how to get the feature importance from pipeline yet\n",
    "\n",
    "feature_importances = my_pipeline.named_steps['model'].feature_importances_\n",
    "\n",
    "# Get the selected feature indices from SelectKBest\n",
    "selected_feature_indices = my_pipeline.named_steps['preprocessor'].\n",
    "print(selected_feature_indices)\n",
    "\n",
    "\n",
    "# # Get the names of the selected features\n",
    "# selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "# # Combine feature names and their importances into a dictionary\n",
    "# feature_importance_dict = dict(zip(selected_feature_names, feature_importances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
